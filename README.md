# âš¡ PerfWattLab â€” GPU Workload Efficiency & Power Evaluation Framework

## ğŸš€ Executive Summary

PerfWattLab is a reproducible GPU workload efficiency evaluation framework built to measure, explain, and improve inference performance and energy consumption for RAG-style LLM workloads.

The goal is simple:

- Measure performance like a systems engineer  
- Measure energy like a power architect  
- Connect software optimization to hardware switching activity  

---

## ğŸ“Š Key Results (Measured)

### âš¡ Inference Performance

- p50 latency reduced from **3.27s â†’ 2.76s** (~15.6% improvement)  
- Throughput improved under optimized execution path  
- Reduced kernel launch overhead by removing high-level pipeline wrapper  

### ğŸ”‹ Energy Efficiency

- Energy per query reduced from **191.5J â†’ 185.3J**  
- Lower average GPU board power under optimized execution  
- Clear latency vs energy Pareto tradeoff curve generated  

### ğŸ§  RTL Switching Activity

- Baseline MAC toggle count: **4219**  
- Optimized MAC toggle count: **2719**  
- **35.5% reduction in switching activity**  

This connects software execution efficiency directly to dynamic power reduction at the silicon level.

---

## ğŸ§  Problem

Modern LLM evaluation focuses heavily on:

- Model accuracy  
- Model size  
- Parameter scaling  

But production systems fail due to:

- Kernel launch inefficiencies  
- GPU underutilization  
- Energy cost per inference  
- Lack of reproducible performance baselines  
- No visibility into perf-per-watt tradeoffs  

PerfWattLab was built to evaluate GPU workloads end-to-end with measurable evidence.

---

## ğŸ— System Overview

```text
Query
  â”‚
  â–¼
RAG Pipeline
  â”œâ”€â”€ FAISS retrieval
  â”œâ”€â”€ Cross-encoder rerank
  â”œâ”€â”€ LLM generation (PyTorch)
  â”‚
  â–¼
Instrumentation Layer
  â”œâ”€â”€ torch.profiler
  â”œâ”€â”€ NVML power sampling
  â”œâ”€â”€ Latency measurement
  â”‚
  â–¼
Experiment Runner
  â”œâ”€â”€ Config sweep
  â”œâ”€â”€ Concurrency variation
  â”œâ”€â”€ Token length variation
  â”‚
  â–¼
Results Artifacts
  â”œâ”€â”€ CSV reports
  â”œâ”€â”€ Power samples
  â”œâ”€â”€ Pareto plots
  â”œâ”€â”€ Profiler traces
```

## ğŸ“ˆ Day-by-Day Engineering Breakdown

### 1ï¸âƒ£ Baseline RAG Execution

- FAISS index with MiniLM embeddings  
- Cross-encoder reranker  
- TinyLlama generator  
- End-to-end timing breakdown  

**Measured:**

- Retrieval ms  
- Rerank ms  
- Generation ms  
- Total latency  

---

### 2ï¸âƒ£ Config Sweep & Concurrency Evaluation

Built a clean experiment runner that sweeps:

- `max_new_tokens`  
- Concurrency  
- Execution path (pipeline vs direct generate)  

**Generated:**

- p50 / p95 latency  
- Throughput (RPS)  
- Tokens/sec  

---

### 3ï¸âƒ£ GPU Profiling & Bottleneck Discovery

Used `torch.profiler` to identify:

- Kernel launch overhead  
- Attention compute dominance  
- CPU-GPU synchronization points  

**Applied fix:**

- Removed high-level pipeline wrapper  
- Used inference mode  
- Explicit device placement  

**Measured improvement:**

15% latency reduction.

---

### 4ï¸âƒ£ Power Measurement & Energy per Query

Instrumented GPU board power using NVML at 5 Hz.

**Computed:**

- Average watts  
- Joules per query  
- Energy per token  

**Generated:**

Latency vs Energy Pareto curve.

**Observation:**

Two configs with similar latency had noticeably different energy profiles.

---

### 5ï¸âƒ£ Triton-Style Evaluation Loop

Simulated production serving conditions:

- Warmup phase  
- Fixed request rate  
- Concurrency sweep  
- Config comparison  

**Generated:**

- Auto markdown report  
- Baseline vs best latency  
- Best throughput  
- Best energy per query  

This mirrors internal evaluation workflows.

---

### 6ï¸âƒ£ RTL Bridge â€” Toggle-Level Power Analysis

To connect software optimization to hardware behavior:

- Wrote baseline Verilog MAC block  
- Implemented optimized operand isolation variant  
- Simulated using `iverilog`  
- Dumped VCD waveforms  
- Counted switching activity via Python  

Initial attempt increased toggles (design mistake).

After redesigning gating logic:

Switching activity reduced by **35.5%**.

Dynamic power âˆ switching activity.

This validates hardware-aware optimization reasoning.

---

## ğŸ“‰ Representative Results

### âš¡ Latency Distribution

- p50: ~2.76s (optimized)  
- p95 reduced relative to baseline  
- Improved tokens/sec  

### ğŸ”‹ Energy

- ~185J per query (optimized)  
- Lower board power average  

### ğŸ§  RTL Switching

| Design    | Total Toggles | Reduction |
|-----------|--------------:|----------:|
| Baseline  | 4219          | â€”         |
| Optimized | 2719          | 35.5%     |

---

## ğŸ§° Tech Stack

| Layer | Technology |
|-------|------------|
| Language | Python |
| Framework | PyTorch |
| Retrieval | FAISS |
| Profiling | torch.profiler |
| Power Metrics | NVML |
| Serving Simulation | Custom evaluation loop |
| RTL Simulation | iverilog |
| Toggle Analysis | Python VCD parser |

---

## ğŸ“‚ Repository Structure

```text
perfwattlab/
â”œâ”€â”€ day1_rag_core.ipynb
â”œâ”€â”€ day2_config_sweep.ipynb
â”œâ”€â”€ day3_profiling.ipynb
â”œâ”€â”€ day4_power_measurement.ipynb
â”œâ”€â”€ day5_eval_loop.ipynb
â”œâ”€â”€ day6_rtl_toggle_fix.ipynb
â”œâ”€â”€ scripts/
â”œâ”€â”€ results/
â”œâ”€â”€ figures/
â””â”€â”€ rtl/
```

## â–¶ï¸ Reproduce

### Install

```bash
pip install -r requirements.txt
```
### Run Core Experiment
```bash
python run_sweep.py
```
### Run Power Sampling
```bash
python power_measurement.py
```
### Run RTL Toggle Analysis
```bash
iverilog mac_baseline.v
iverilog mac_optimized.v
python toggle_counter.py
```

## ğŸ“Š Figures

### Latency vs Energy
![Latency vs Energy](figures/latency_vs_energy.png)

---

### Before Optimization
![Before Optimization](figures/before_direct.png)

---

### After Optimization
![After Optimization](figures/after_direct.png)

---

### RTL Toggle Comparison
![RTL Toggle Comparison](figures/rtl_toggle_comparison.png)


## âš ï¸ Limitations

- Evaluated on Colab T4 GPU

- NVML sampling at 5 Hz (not high-resolution power rail)

- RTL power estimated via toggle count (not signoff tool like PrimeTime PX)

- Single-node setup

## ğŸ§­ Future Work

- Integrate Triton Inference Server directly

- Add DCGM exporter + Prometheus integration

- Extend to multi-GPU workloads

- Add automated power-performance modeling

## ğŸ‘¤ Author

Rudra Brahmbhatt
ML Infrastructure & MLOps Engineer 
